{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.0, 66.0, 2.0, 1.0, 89.0]\n",
      "0\n",
      "53\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Pre-process the data\n",
    "training_data = []\n",
    "\n",
    "# Set up tokenizer, bag of words, and make a defaultdict to hold a tuple of (tweets_text, [scores]) for each\n",
    "# Loop through the files\n",
    "tokenizer = TweetTokenizer(strip_handles=True)\n",
    "tweets_and_scores = defaultdict(tuple)\n",
    "#bag_of_words = []\n",
    "word_dict = defaultdict(int)\n",
    "\n",
    "# Grab the scores\n",
    "scores = pd.DataFrame.from_csv('scraped_twitter_score.csv')\n",
    "print(list(scores.ix[\"lordhamstr\"]))\n",
    "\n",
    "handles_with_scores = list(scores.index)\n",
    "# Grab the tweets and scores\n",
    "for tweet_file in os.listdir('statuses'):\n",
    "    if tweet_file.endswith('_output.txt'):\n",
    "        # Get the handle\n",
    "        beginning = 9\n",
    "        ending = tweet_file.index('_output.txt')\n",
    "        handle = tweet_file[beginning:ending]\n",
    "        \n",
    "        if handle in handles_with_scores:\n",
    "            # Put the text and scores in a dictionary\n",
    "            with open('statuses/' + tweet_file, 'r', encoding='utf8') as tweet_text:\n",
    "                score = list(scores.ix[handle])\n",
    "                text = tokenizer.tokenize(tweet_text.read().replace('\\nTWEETLINEBREAK\\n', ' '))\n",
    "                #bag_of_words += text\n",
    "                #for word in text:\n",
    "                #    word_dict[word] += 1\n",
    "                tweets_and_scores[handle] = (text, score)\n",
    "            \n",
    "#bag_of_words = set(bag_of_words)\n",
    "print(len(word_dict))\n",
    "#bag_of_words = {word:n for word,n in word_dict.items() if n in range(40,100)}.keys()\n",
    "#bag_of_words = list({word:n for word,n in word_dict.items() if n > 2}.keys())\n",
    "bag_of_words = ['interview', 'I', 'video', '😊', 'lol', 'Boughton', '(', 'na', 'beer', 'and', '🐍', ':', 'Singapore', 'fucking', 'on', '-', '?', '#songoftheday', 'of', 'a', \"I'm\", 'you', 'that', '\"', '/', 'i', \"'\", ')', 'single', 'me', ';', 'for', 'The', '’', '*', 'my', '#seizulogic', 'like', 'am', 'Facebook', 'o', '❤', 'ProKabaddi', '#seizuBOTty', 'is', '.', '🔥', ',', '..', 'LOL', 'your', '...', '!']\n",
    "num_words = len(bag_of_words)\n",
    "print(num_words)\n",
    "num_observations = len(tweets_and_scores)\n",
    "\n",
    "# From this, you have a dictionary called tweets_and_scores and a list called bag_of_words\n",
    "\n",
    "# Convert tweets_texts into vector of words\n",
    "\n",
    "# Make X matrix of size [len(trainingdata), bag_of_words_length]\n",
    "X = np.zeros([num_observations, num_words + 1])\n",
    "\n",
    "# Make Y matrix of size [len(trainingdata), 5] to hold personality scores\n",
    "Y_array = [np.zeros([num_observations, 1]) for x in range(0,5)]\n",
    "\n",
    "word_positions = defaultdict(int)\n",
    "index = 0\n",
    "for word in bag_of_words:\n",
    "    word_positions[word] = index\n",
    "    index += 1\n",
    "\n",
    "# Fill them\n",
    "index = 0\n",
    "for (key, (words, scores)) in tweets_and_scores.items():\n",
    "    word_index = 0\n",
    "    word_length = 0\n",
    "    for word in words:\n",
    "        X[index][word_positions[word]] += 1\n",
    "        word_length += len(word)\n",
    "        \n",
    "    X[index][-1] = word_length / len(words)\n",
    "    \n",
    "    score_index = 0\n",
    "    for score in scores:\n",
    "        Y_array[score_index][index] = score\n",
    "        score_index += 1\n",
    "    \n",
    "    print(index)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train the model\n",
    "### This is for quickly testing different score thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "for (key, (words, scores)) in tweets_and_scores.items():\n",
    "    score_index = 0\n",
    "    for score in scores:\n",
    "        if score > 50:\n",
    "            Y_array[score_index][index] = 1\n",
    "        else:\n",
    "            Y_array[score_index][index] = 0\n",
    "        score_index += 1\n",
    "    \n",
    "    print(index)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            SVR  OLS Lasso\n",
      "MAE    0.605263  NaN   NaN\n",
      "MSQE       0.75  NaN   NaN\n",
      "MedAE  0.724138  NaN   NaN\n",
      "R2     0.736842  NaN   NaN\n",
      "\n",
      "            SVR  OLS Lasso\n",
      "MAE    0.447368  NaN   NaN\n",
      "MSQE   0.521739  NaN   NaN\n",
      "MedAE  0.545455  NaN   NaN\n",
      "R2     0.533333  NaN   NaN\n",
      "\n",
      "            SVR  OLS Lasso\n",
      "MAE    0.605263  NaN   NaN\n",
      "MSQE   0.590909  NaN   NaN\n",
      "MedAE  0.684211  NaN   NaN\n",
      "R2     0.634146  NaN   NaN\n",
      "\n",
      "            SVR  OLS Lasso\n",
      "MAE    0.657895  NaN   NaN\n",
      "MSQE   0.785714  NaN   NaN\n",
      "MedAE  0.758621  NaN   NaN\n",
      "R2      0.77193  NaN   NaN\n",
      "\n",
      "            SVR  OLS Lasso\n",
      "MAE    0.657895  NaN   NaN\n",
      "MSQE       0.65  NaN   NaN\n",
      "MedAE  0.684211  NaN   NaN\n",
      "R2     0.666667  NaN   NaN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import KFold, cross_val_score, cross_val_predict\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df_o = pd.DataFrame(columns=[\"SVR\", \"OLS\", \"Lasso\"], index=[\"MAE\", \"MSQE\", \"MedAE\", \"R2\"])\n",
    "df_c = pd.DataFrame(columns=[\"SVR\", \"OLS\", \"Lasso\"], index=[\"MAE\", \"MSQE\", \"MedAE\", \"R2\"])\n",
    "df_e = pd.DataFrame(columns=[\"SVR\", \"OLS\", \"Lasso\"], index=[\"MAE\", \"MSQE\", \"MedAE\", \"R2\"])\n",
    "df_a = pd.DataFrame(columns=[\"SVR\", \"OLS\", \"Lasso\"], index=[\"MAE\", \"MSQE\", \"MedAE\", \"R2\"])\n",
    "df_n = pd.DataFrame(columns=[\"SVR\", \"OLS\", \"Lasso\"], index=[\"MAE\", \"MSQE\", \"MedAE\", \"R2\"])\n",
    "\n",
    "dfs = [df_o, df_c, df_e, df_a, df_n]\n",
    "\n",
    "n_folds = 4\n",
    "k_fold = KFold(num_observations, n_folds=n_folds, shuffle=True, random_state=0)\n",
    "\n",
    "if False:\n",
    "\n",
    "    index = 0\n",
    "    for Y in Y_array:\n",
    "        mnb = SVR()\n",
    "        predictions = cross_val_predict(mnb, X, Y.ravel(), cv=4)\n",
    "        dfs[index][\"SVR\"][0] = metrics.mean_absolute_error(Y, predictions)\n",
    "        dfs[index][\"SVR\"][1] = metrics.mean_squared_error(Y, predictions)\n",
    "        dfs[index][\"SVR\"][2] = metrics.median_absolute_error(Y, predictions)\n",
    "        dfs[index][\"SVR\"][3] = metrics.r2_score(Y, predictions)\n",
    "\n",
    "        mnb = LinearRegression()\n",
    "        predictions = cross_val_predict(mnb, X, Y.ravel(), cv=4)\n",
    "        dfs[index][\"OLS\"][0] = metrics.mean_absolute_error(Y, predictions)\n",
    "        dfs[index][\"OLS\"][1] = metrics.mean_squared_error(Y, predictions)\n",
    "        dfs[index][\"OLS\"][2] = metrics.median_absolute_error(Y, predictions)\n",
    "        dfs[index][\"OLS\"][3] = metrics.r2_score(Y, predictions)\n",
    "\n",
    "        y = Y\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(y, predictions)\n",
    "        ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "        ax.set_xlabel('Measured')\n",
    "        ax.set_ylabel('Predicted')\n",
    "        plt.show()\n",
    "\n",
    "        #mnb = Lasso(max_iter=10000)\n",
    "        #predictions = cross_val_predict(mnb, X, Y.ravel(), cv=4)\n",
    "        dfs[index][\"Lasso\"][0] = metrics.mean_absolute_error(Y, predictions)\n",
    "        dfs[index][\"Lasso\"][1] = metrics.mean_squared_error(Y, predictions)\n",
    "        dfs[index][\"Lasso\"][2] = metrics.median_absolute_error(Y, predictions)\n",
    "        dfs[index][\"Lasso\"][3] = metrics.r2_score(Y, predictions)\n",
    "        index += 1\n",
    "\n",
    "\n",
    "        svr = SVR()\n",
    "        ols = LinearRegression()\n",
    "        lasso = Lasso(max_iter=5000)\n",
    "\n",
    "        print(X.shape)\n",
    "        print()\n",
    "\n",
    "    index = 0\n",
    "    for Y in Y_array:\n",
    "        #lasso.fit(X,Y.ravel())\n",
    "        #model = SelectFromModel(lasso, prefit=True)\n",
    "        #X_new = model.transform(X)\n",
    "\n",
    "        #lasso.fit(X, Y.ravel())\n",
    "        svr.fit(X,Y.ravel())\n",
    "        ols.fit(X,Y.ravel())\n",
    "        predict_svr = svr.predict(X)\n",
    "        predict_ols = ols.predict(X)\n",
    "        #predict_lasso = lasso.predict(X)\n",
    "\n",
    "\n",
    "        dfs[index][\"SVR\"][0] = metrics.mean_absolute_error(Y, predict_svr)\n",
    "        dfs[index][\"SVR\"][1] = metrics.mean_squared_error(Y, predict_svr)\n",
    "        dfs[index][\"SVR\"][2] = metrics.median_absolute_error(Y, predict_svr)\n",
    "        dfs[index][\"SVR\"][3] = metrics.r2_score(Y, predict_svr)\n",
    "\n",
    "        dfs[index][\"OLS\"][0] = metrics.mean_absolute_error(Y, predict_ols)\n",
    "        dfs[index][\"OLS\"][1] = metrics.mean_squared_error(Y, predict_ols)\n",
    "        dfs[index][\"OLS\"][2] = metrics.median_absolute_error(Y, predict_ols)\n",
    "        dfs[index][\"OLS\"][3] = metrics.r2_score(Y, predict_ols)\n",
    "\n",
    "        dfs[index][\"Lasso\"][0] = metrics.mean_absolute_error(Y, predict_lasso)\n",
    "        dfs[index][\"Lasso\"][1] = metrics.mean_squared_error(Y, predict_lasso)\n",
    "        dfs[index][\"Lasso\"][2] = metrics.median_absolute_error(Y, predict_lasso)\n",
    "        dfs[index][\"Lasso\"][3] = metrics.r2_score(Y, predict_lasso)\n",
    "\n",
    "        index += 1\n",
    "\n",
    "    #print(list(map(lambda x: bag_of_words[x], model.get_support(indices=True))))\n",
    "\n",
    "index = 0\n",
    "Y_array_new = [np.zeros([num_observations, 1]) for x in range(0,5)]\n",
    "for i in range(len(Y_array_new)):\n",
    "    Y_array_new[i] = [y > 50 for y in Y_array[i]]\n",
    "    \n",
    "    mnb = LinearRegression()\n",
    "    predictions = cross_val_predict(mnb, X, Y_array[i].ravel(), cv=4)\n",
    "    prediction_new = [pred > 50 for pred in predictions]\n",
    "    dfs[index][\"SVR\"][0] = metrics.accuracy_score(Y_array_new[i], prediction_new)\n",
    "    dfs[index][\"SVR\"][1] = metrics.precision_score(Y_array_new[i], prediction_new)\n",
    "    dfs[index][\"SVR\"][2] = metrics.recall_score(Y_array_new[i], prediction_new)\n",
    "    dfs[index][\"SVR\"][3] = metrics.f1_score(Y_array_new[i], prediction_new)\n",
    "\n",
    "    index += 1\n",
    "    \n",
    "\n",
    "print()\n",
    "for df in dfs:\n",
    "    print(df)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['regression__bag_of_words.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "lookup = \"ocean\"\n",
    "index = 0\n",
    "for Y in Y_array:\n",
    "    mnb = LinearRegression()\n",
    "    mnb.fit(X, Y.ravel())\n",
    "    coefs = mnb.coef_[0]\n",
    "\n",
    "    bag = list(bag_of_words)\n",
    "\n",
    "    # Use initial model counts for all but extroversion\n",
    "    joblib.dump(mnb, 'regression_' + lookup[index] + '.pkl')\n",
    "    index += 1\n",
    "joblib.dump(list(bag_of_words), 'regression__bag_of_words.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
